{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modules.text\n",
    "\n",
    "# export\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "import typing\n",
    "from abc import abstractmethod\n",
    "import nltk\n",
    "import traitlets\n",
    "from traitlets import HasTraits, Dict, Any, Unicode, Integer, Bytes, Instance, Bool\n",
    "from dharpa_toolbox.modules.core import DharpaModule\n",
    "from dharpa_toolbox.modules.files import DharpaFiles, TextCorpus\n",
    "from dharpa_toolbox.workflows import DharpaWorkflow\n",
    "\n",
    "\n",
    "class TextPreprocessSettingsModule(DharpaModule):\n",
    "\n",
    "    _module_name = \"text_preprocess_settings\"\n",
    "\n",
    "    def _create_inputs(self, **config) -> HasTraits:\n",
    "\n",
    "        class TextPreprocessingInput(HasTraits):\n",
    "            file_set = Instance(klass=DharpaFiles, allow_none=True)\n",
    "            lowercase = Bool(default_value=True)\n",
    "\n",
    "        return TextPreprocessingInput()\n",
    "\n",
    "    def _create_outputs(self, **config) -> HasTraits:\n",
    "\n",
    "        class TextPreprocessingSettings(HasTraits):\n",
    "            settings = Dict(allow_none=True)\n",
    "\n",
    "        return TextPreprocessingSettings()\n",
    "\n",
    "    def _process(self, **inputs) -> typing.Mapping[str, typing.Any]:\n",
    "\n",
    "        return {\"settings\": {\"lowercase\": inputs[\"lowercase\"]}}\n",
    "\n",
    "\n",
    "class TextPreprocessingModule(DharpaModule):\n",
    "\n",
    "    _module_name = \"text_preprocessing\"\n",
    "\n",
    "    def _create_inputs(self, **config) -> HasTraits:\n",
    "\n",
    "        class TextPreprocessingInput(HasTraits):\n",
    "            file_set = Instance(klass=DharpaFiles, allow_none=True)\n",
    "            settings = Dict(allow_none=True)\n",
    "\n",
    "        return TextPreprocessingInput()\n",
    "\n",
    "    def _create_outputs(self, **config):\n",
    "\n",
    "        class TextPreprocessingOutput(HasTraits):\n",
    "            preprocessed_text = Dict(allow_none=True)\n",
    "\n",
    "        return TextPreprocessingOutput()\n",
    "\n",
    "    def _process(self, **inputs) -> typing.Mapping[str, typing.Any]:\n",
    "\n",
    "        result = {}\n",
    "        file_set: DharpaFiles = inputs[\"file_set\"]\n",
    "        if file_set is None:\n",
    "            file_set = DharpaFiles()\n",
    "\n",
    "        for f in file_set.files:\n",
    "            result[f.name] = f.content.lower()\n",
    "\n",
    "        return result\n",
    "\n",
    "class TextCorpusValue(HasTraits):\n",
    "\n",
    "    text_corpus = Instance(klass=TextCorpus)\n",
    "\n",
    "class TextCorpusInputValue(TextCorpusValue):\n",
    "\n",
    "    enabled = Bool(default_value=True)\n",
    "\n",
    "\n",
    "\n",
    "class TokenizeTextModule(DharpaModule):\n",
    "\n",
    "    _module_name = \"tokenize\"\n",
    "\n",
    "    def _create_inputs(self, **config) -> HasTraits:\n",
    "\n",
    "        class TokenizeTextInput(HasTraits):\n",
    "            file_set = Instance(klass=DharpaFiles)\n",
    "\n",
    "        return TokenizeTextInput()\n",
    "\n",
    "    def _create_outputs(self, **config) -> HasTraits:\n",
    "\n",
    "        return TextCorpusValue()\n",
    "\n",
    "    def _process(self, **inputs) -> typing.Mapping[str, typing.Any]:\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        corpus: TextCorpus = inputs.pop(\"text_corpus\")\n",
    "\n",
    "        for id, text in corpus.corpus().items():\n",
    "\n",
    "            r = nltk.wordpunct_tokenize(text)\n",
    "            # TODO: check if iterable of strings\n",
    "            if r:\n",
    "                result[id] = r\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class TextCorpusProcessingModule(DharpaModule):\n",
    "\n",
    "    @abstractmethod\n",
    "    def _process_tokens(self, token_set: typing.List[str], config: typing.Mapping[str, Any]) -> str:\n",
    "        pass\n",
    "\n",
    "    def _create_inputs(self, **config) -> HasTraits:\n",
    "\n",
    "        return TextCorpusInputValue()\n",
    "\n",
    "    def _create_outputs(self, **config) -> HasTraits:\n",
    "\n",
    "        return TextCorpusValue()\n",
    "\n",
    "    def _process(self, **inputs) -> typing.Mapping[str, typing.Any]:\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        config = copy.copy(inputs)\n",
    "        token_sets = config.pop(\"token_sets\")\n",
    "\n",
    "        for id, token_set in token_sets.items():\n",
    "\n",
    "            r = self._process_text(token_set=token_set, config=config)\n",
    "            if r:\n",
    "                result[id] = r\n",
    "\n",
    "        return {\"token_sets\": result}\n",
    "\n",
    "\n",
    "class LowercaseTextModule(TextCorpusProcessingModule):\n",
    "\n",
    "    _module_name = \"lowercase\"\n",
    "\n",
    "    def _process_tokens(self, token_set: typing.List[str], config: typing.Mapping[str, Any]) -> str:\n",
    "\n",
    "        return [x.lower() for x in token_set]\n",
    "\n",
    "\n",
    "class RemoveStopwordsModule(TextCorpusProcessingModule):\n",
    "\n",
    "    _module_name = \"remove_stopwords\"\n",
    "\n",
    "    def _create_inputs(self, **config) -> HasTraits:\n",
    "\n",
    "        class RemoveStopwordsInput(TextCorpusInputValue):\n",
    "            stopwords = traitlets.List()\n",
    "\n",
    "        return RemoveStopwordsInput()\n",
    "\n",
    "    def _process_tokens(self, token_set: typing.List[str], config: typing.Mapping[str, Any]) -> typing.Mapping[str, typing.Any]:\n",
    "\n",
    "        stopwords = config[\"stopwords\"]\n",
    "\n",
    "        return [x for x in token_set if x not in stopwords]\n",
    "\n",
    "\n",
    "\n",
    "class CorpusProcessingWorkflow(DharpaWorkflow):\n",
    "\n",
    "    _module_name = \"corpus_processing\"\n",
    "\n",
    "    def __init__(self, **config):\n",
    "\n",
    "        modules = [\n",
    "            {\n",
    "                \"type\": \"tokenize\",\n",
    "                \"id\": \"tokenize_corpus\",\n",
    "                \"input_map\": {\n",
    "                    \"file_set\": {\n",
    "                        \"value_name\": \"file_set\"\n",
    "                    },\n",
    "\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"lowercase\",\n",
    "                \"id\": \"lowercase_corpus\",\n",
    "                \"input_map\": {\n",
    "                    \"text_corpus\": {\n",
    "                        \"module\": \"tokenize_corpus\",\n",
    "                        \"value_name\": \"text_corpus\"\n",
    "\n",
    "                    },\n",
    "                    \"enabled\": {\n",
    "                        \"value_name\": \"enable_lowercase\"\n",
    "                    }\n",
    "\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"remove_stopwords\",\n",
    "                \"id\": \"remove_stopwords_from_corpus\",\n",
    "                \"input_map\": {\n",
    "                    \"text_corpus\": \"lowercase_corpus\",\n",
    "                    \"stopwords\": {\n",
    "                        \"value_name\": \"stopwords\"\n",
    "                    },\n",
    "                    \"enabled\": {\n",
    "                        \"value_name\": \"enable_stopwords_removal\"\n",
    "                    }\n",
    "                },\n",
    "                \"output_map\": {\n",
    "                    \"text_corpus\": \"processed_text_corpus\"\n",
    "\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        super().__init__(modules=modules, **config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_data_types.ipynb.\n",
      "Converted 02_modules_core.ipynb.\n",
      "Converted 03_data_sources.ipynb.\n",
      "Converted 03_data_targets.ipynb.\n",
      "Converted 05_workflows.ipynb.\n",
      "Converted 10_modules_files.ipynb.\n",
      "Converted 10_modules_text.ipynb.\n",
      "Converted 99_utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from dharpa_toolbox.core import export_notebooks\n",
    "export_notebooks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
